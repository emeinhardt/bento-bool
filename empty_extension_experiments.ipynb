{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def heaviside(x):\n",
    "    return np.array(1 * (x >= 0))\n",
    "def powerset(iterable):\n",
    "    \"powerset([1,2,3]) --> () (1,) (2,) (3,) (1,2) (1,3) (2,3) (1,2,3)\"\n",
    "    s = list(iterable)\n",
    "    return list(itertools.chain.from_iterable(itertools.combinations(s, r) for r in range(len(s)+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trinary_hash_function(p):\n",
    "    \"\"\"\n",
    "    Takes in a vector of in {-1,0,1}^M and returns the unique associated integer\n",
    "    Input:\n",
    "        p - a vector (in general an iterable) of length M with entries in {-1,0,1}\n",
    "    Output:\n",
    "        hash_value - the unique integer in [0,...,3^M -1] associated with p\n",
    "    \"\"\"\n",
    "    M = len(p)\n",
    "    hash_value = 0\n",
    "    for i in range(M):\n",
    "        hash_value += (p[i]+1)*(3**i)\n",
    "    return hash_value\n",
    "def get_children(p):\n",
    "    \"\"\"\n",
    "    Given a partially specified feature vector p with K > 0 unspecified elements, return the 2K children obtained\n",
    "    by picking one of the nonspecified elements and replacing it with +/- 1.\n",
    "    \n",
    "    Input:\n",
    "        p - a vector of shape (M,) with elements from {-1,0,1}.  The partially specified feature vector\n",
    "    Output:\n",
    "        children - a list of length 2K, all vectors that are consistent with p and one degree more specified\n",
    "    \"\"\"\n",
    "    \n",
    "    M = len(p)\n",
    "    zero_inds = [i for i in range(M) if p[i]==0]\n",
    "    K = len(zero_inds)\n",
    "    children = []\n",
    "    for ind in zero_inds:\n",
    "        q_plus = list(p)\n",
    "        q_plus[ind]=1\n",
    "        children.append(tuple(q_plus))\n",
    "        q_minus = list(p)\n",
    "        q_minus[ind]=-1\n",
    "        children.append(tuple(q_minus))\n",
    "        \n",
    "    return children"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_empty_kernel(V):\n",
    "    \"\"\"\n",
    "    Returns the list of all psfvs that have empty extension with respect to V\n",
    "    \n",
    "    Input:\n",
    "        V-  a matrix of shape (L,M) with elements from {-1,1}.  The feature vectors\n",
    "    Output:\n",
    "        R - a list of partially specified feature vectors such that all elements of R have empty extension in V\n",
    "            but for any vector in R, flipping any specified element to zero will cause the resulting psfv to have\n",
    "            nonempty extension in V.\n",
    "    \"\"\"\n",
    "    \n",
    "    L,M = np.shape(V)\n",
    "    \n",
    "    visited = set() #this will hold the integer hashes of all visited psfvs\n",
    "    R = [] #the list of target psfvs\n",
    "    \n",
    "    return R "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def fast_reducer(subsets):\n",
    "    \"\"\"\n",
    "    This is a slightly more efficient algorithm for pruning a set of subsets so that any subset J that is a superset\n",
    "    of any other distinct subset J_prime is removed \n",
    "    \n",
    "    Input:\n",
    "        subsets - an iterable of subsets of indices\n",
    "    Output:\n",
    "        reduced_subsets - a possibly smaller set of subsets\n",
    "    \"\"\"\n",
    "    \n",
    "    subset_list = sorted(list(subsets),key=lambda J: len(J))\n",
    "    #we start with the smallest subset because it can't possibly be a superset of any other distinct subset\n",
    "    reduced_subsets = [subset_list[0]]\n",
    "    for i in range(1,len(subset_list)):\n",
    "        J = subset_list[i]\n",
    "        #because subset_list is sorted by size, we know that J can't be a strict subset \n",
    "        #of any current element of of reduced_subsets\n",
    "        retain = True\n",
    "        for J_prime in reduced_subsets:\n",
    "            if J > J_prime:\n",
    "                retain=False\n",
    "                break\n",
    "        if retain:\n",
    "            reduced_subsets.append(J)\n",
    "    return set(reduced_subsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def extension_bool(p,V):\n",
    "    \"\"\"\n",
    "    Compute a boolean vector that represents the extension of p in V\n",
    "    \n",
    "    Inputs:\n",
    "        p - a vector of shape (M,) with elements from {-1,0,1}.  The partially specified feature vector\n",
    "        V-  a matrix of shape (L,M) with elements from {-1,1}.  The feature vectors\n",
    "    Outputs:\n",
    "        extension - a vector of shape (L) with elements from {1,0}.  extension[l]=1 iff V[l,:] is in the extension of p\n",
    "    \"\"\"\n",
    "    L = np.shape(V)[0]\n",
    "    if np.sum(abs(p)) == 0:\n",
    "        #the all zeros vector\n",
    "        return np.ones(L)\n",
    "    else:\n",
    "        E = np.dot(V,p)/np.sum(abs(p))\n",
    "        return heaviside(E-1)\n",
    "\n",
    "def extension_multi_bool(p_mat,V):\n",
    "    \"\"\"\n",
    "    Compute a boolean vector that represents the extension of p in V\n",
    "    \n",
    "    Inputs:\n",
    "        p_mat - a matrix of shape (M,num_p) with elements from {-1,0,1}.  The matrix of partially specified\n",
    "            feature vectors, containing num_p vectors\n",
    "        V-  a matrix of shape (L,M) with elements from {-1,1}.  The feature vectors\n",
    "    Outputs:\n",
    "        extension - a matrix of shape (L,num_p) with elements from {1,0}.  extension[l,i]=1 iff V[l,:] is \n",
    "            in the extension of p_mat[:,i]\n",
    "    \"\"\"\n",
    "    K_vec = np.sum(abs(p_mat),axis=0) #shape is (num_p,)\n",
    "    E = np.dot(V,p_mat) #shape is (L,num_p)\n",
    "    return heaviside(E-K_vec[np.newaxis,:])\n",
    "\n",
    "def extension_mat(p,V):\n",
    "    \"\"\"\n",
    "    Returns a matrix representing the extension of p in V\n",
    "    \n",
    "    Inputs:\n",
    "        p - a vector of shape (M,) with elements from {-1,0,1}.  The partially specified feature vector\n",
    "        V-  a matrix of shape (L,M) with elements from {-1,1}.  The feature vectors\n",
    "    Outputs:\n",
    "        A - A matrix of shape (N,M) with elements from {-1,1}, where N <=L, and the rows of A are the rows of V in the\n",
    "            extension of p\n",
    "    \"\"\"\n",
    "    \n",
    "    bool_vec = extension_bool(p,V)\n",
    "    return V[bool_vec>0,:]\n",
    "\n",
    "def check_equivalent(p,q,V):\n",
    "    \"\"\"\n",
    "    Checks if p and q have the same extension in V\n",
    "    \n",
    "    Inputs:\n",
    "        p,q - vectors of shape (M,) with elements from {-1,0,1}.  The partially specified feature vectors\n",
    "        V-  a matrix of shape (L,M) with elements from {-1,1}.  The feature vectors\n",
    "    Outputs:\n",
    "        match - bool, whether or not p and q have the same extension \n",
    "    \"\"\"\n",
    "    p_bool = extension_bool(p,V)\n",
    "    q_bool = extension_bool(q,V)\n",
    "    M = np.shape(q_bool)[0]\n",
    "    return all([p_bool[i]==q_bool[i] for i in range(M)])\n",
    "\n",
    "def mutate_p(p,abstraction_set,specification_set):\n",
    "    \"\"\"\n",
    "    Returns q that is equal to p with mutations (specifications and generalizations applied)\n",
    "    \n",
    "    Inputs:\n",
    "        p - the partially specified feature vector to be mutated\n",
    "        abstraction_set - a set of indices to flip currently specified values of p to zero\n",
    "        specification_set - a set of tuples (index,value) to change currently unspecified values of p to the given value\n",
    "    Outputs:\n",
    "        q - the mutated vector\n",
    "    \"\"\"\n",
    "    \n",
    "    q = np.copy(p)\n",
    "    for abstraction in abstraction_set:\n",
    "        q[abstraction]=0\n",
    "    for specification_tuple in specification_set:\n",
    "        q[specification_tuple[0]]=specification_tuple[1]\n",
    "    return q\n",
    "\n",
    "def get_allowed_specifications(p,V):\n",
    "    \"\"\"\n",
    "    Given a set of feature vectors V and a query partially specified feature vector p, all locations where \n",
    "    p can be specified (flipped from a zero to +/- 1) without changing the extension in V.\n",
    "    \n",
    "    Inputs:\n",
    "        p - a vector of shape (M,) with elements from {-1,0,1}.  The partially specified feature vector\n",
    "        V -  a matrix of shape (L,M) with elements from {-1,1}.  The feature vectors\n",
    "    Outputs:\n",
    "        allowed_specifications - a list of tuples of form (index,value) where p[index] can be set to value\n",
    "        without changing extension\n",
    "    \"\"\"\n",
    "    \n",
    "    M = np.shape(p)[0]\n",
    "    \n",
    "    bool_p = extension_bool(p,V)\n",
    "    \n",
    "    A = V[bool_p>0,:]#shape is (N,M)\n",
    "    N = np.shape(A)[0]\n",
    "    A_sums = np.sum(A,axis=0)\n",
    "    \n",
    "    allowed_specifications = []\n",
    "    for i in range(M):\n",
    "        if p[i]==0:\n",
    "            #we can flip this position to a signed value if only if all A[:,i] or equivalently A_sums[i]== +/- N\n",
    "            if N==0:\n",
    "                #empty extension means we can replace all zeros with +1 or -1\n",
    "                allowed_specifications.append((i,1))\n",
    "                allowed_specifications.append((i,-1))              \n",
    "            elif A_sums[i]==N:\n",
    "                allowed_specifications.append((i,1)) #they're all 1 so we can mutate p[i] to 1\n",
    "            elif A_sums[i]==-N:\n",
    "                allowed_specifications.append((i,-1)) #they're all -1 so we can mutate p[i] to -1\n",
    "    return allowed_specifications\n",
    "    \n",
    "def get_allowed_abstraction_sets(p,V):\n",
    "    \"\"\"\n",
    "    Given a set of feature vectors V and a query partially specified feature vector p, return all partially\n",
    "    specified vectors q that have the same extension in V as p.\n",
    "    \n",
    "    Inputs:\n",
    "        p - a vector of shape (M,) with elements from {-1,0,1}.  The partially specified feature vector\n",
    "        V -  a matrix of shape (L,M) with elements from {-1,1}.  The feature vectors\n",
    "    Outputs:\n",
    "        allowed_abstraction_sets - a list of sets of indices, where each set denotes a set of indices of p\n",
    "            where a previously specified value can be flipped to zero without changing extension\n",
    "    \"\"\"\n",
    "    bool_p = extension_bool(p,V)\n",
    "    B = V[bool_p<=0,:]#shape is (T,M), p is not compatible with any vector in B\n",
    "    (T,M) = np.shape(B)\n",
    "    \n",
    "    \n",
    "    prespecified_indices = [i for i in range(M) if p[i] != 0]\n",
    "    if T==0:\n",
    "        #a special case when the extension of p is all of V, and thus we can zero out any combination of indices\n",
    "        return set([frozenset(J) for J in powerset(prespecified_indices)])\n",
    "    \n",
    "    #now we find all subsets J of the specified indices such that if we zeroed out every index of p contained in J, the\n",
    "    #new vector would be consistent with an element of B\n",
    "    #this accounts to looking at each element b (row) of B, and finding all the indices where p disagrees \n",
    "    #with that element and adding that set of indices to an accumulating set\n",
    "    forbidden_index_subsets_ = []\n",
    "    \n",
    "    for t_val in range(T):\n",
    "        clash_points = []\n",
    "        for m_val in prespecified_indices:\n",
    "            if B[t_val,m_val]*p[m_val] == -1:\n",
    "                clash_points.append(m_val)\n",
    "        forbidden_index_subsets_.append(frozenset(clash_points))\n",
    "    forbidden_index_subsets = set(forbidden_index_subsets_)\n",
    "    \n",
    "    \"\"\"\n",
    "    We can trim this down though by noting that any element of forbidden_index_subsets that is a \n",
    "    proper superset of another element of forbidden_index subsets is redundant, as we will be building\n",
    "    up sets of indices that are not supersets of of any element of forbidden_index subsets.\n",
    "    \n",
    "    \n",
    "    reduced_forbidden_index_subsets = set()\n",
    "    for J in forbidden_index_subsets:\n",
    "        retain = True\n",
    "        for J_prime in forbidden_index_subsets:\n",
    "            if J > J_prime: #J is a superset of J_prime\n",
    "                retain=False\n",
    "                break\n",
    "        if retain:\n",
    "            reduced_forbidden_index_subsets.add(J)\n",
    "    \"\"\"\n",
    "    reduced_forbidden_index_subsets=fast_reducer(forbidden_index_subsets)\n",
    "    #now we try and build up all the legal subsets of the prespecified indices using a sort of breadth first\n",
    "    #search.  We start with all the legal singletons\n",
    "    \n",
    "    allowed_abstraction_sets=set()\n",
    "    my_queue = []\n",
    "    allowed_singletons = []\n",
    "    for i in prespecified_indices:\n",
    "        trial_singleton = frozenset([i])\n",
    "        if not trial_singleton in reduced_forbidden_index_subsets:\n",
    "            my_queue.append(trial_singleton)\n",
    "            allowed_singletons.append(trial_singleton)\n",
    "    \n",
    "    #now we will build up allowed sets from elements of the queue\n",
    "    \n",
    "    while len(my_queue)>0:\n",
    "        trial_set = my_queue.pop(0)\n",
    "        if not any([trial_set >= J for J in reduced_forbidden_index_subsets]):\n",
    "            #we haven't accidentally included any of the forbidden index subsets as a a subset of trial_set\n",
    "            allowed_abstraction_sets.add(trial_set)\n",
    "            #now we combine this allowed set with all the allowed singletons it doesn't contain and add to queue\n",
    "            #for later processing\n",
    "            for trial_singleton in allowed_singletons:\n",
    "                if not (trial_set >= trial_singleton): #the trial set doesn't already contain this singleton\n",
    "                    new_set = trial_set.union(trial_singleton)\n",
    "                    my_queue.append(new_set)\n",
    "            \n",
    "    allowed_abstraction_sets.add(frozenset()) #since not abstracting any indices is a legal mutation\n",
    "    return allowed_abstraction_sets#, B,forbidden_index_subsets_\n",
    "    \n",
    "def equivalence_class(p,V):\n",
    "    \"\"\"\n",
    "    Given a set of feature vectors V and a query partially specified feature vector p, return all partially\n",
    "    specified vectors q that have the same extension in V as p.\n",
    "    \n",
    "    Inputs:\n",
    "        p - a vector of shape (M,) with elements from {-1,0,1}.  The partially specified feature vector\n",
    "        V-  a matrix of shape (L,M) with elements from {-1,1}.  The feature vectors\n",
    "    Outputs:\n",
    "        R - a list of vectors where R[i] has shape (M,) and all vectors in R have the same extension in V as p\n",
    "    \"\"\"\n",
    "    \n",
    "    M = np.shape(p)[0]\n",
    "    L = np.shape(V)[0]\n",
    "    V_sums = np.sum(V,axis=0)\n",
    "    bool_p = extension_bool(p,V)\n",
    "    \n",
    "    if any(bool_p):\n",
    "        #nonempty extension\n",
    "        allowed_specifications = get_allowed_specifications(p,V)\n",
    "        most_specific_q = np.copy(p)\n",
    "        for specification in allowed_specifications:\n",
    "            most_specific_q[specification[0]] = specification[1]\n",
    "    \n",
    "        #print most_specific_q\n",
    "        allowed_abstraction_sets = get_allowed_abstraction_sets(most_specific_q,V)\n",
    "    \n",
    "        equivalence_class = []\n",
    "    \n",
    "        for abstraction_set in allowed_abstraction_sets:\n",
    "            q = np.copy(most_specific_q)\n",
    "            for abstraction in abstraction_set:\n",
    "                q[abstraction]=0\n",
    "            equivalence_class.append(q)\n",
    "    \n",
    "    else:\n",
    "        #the extension is empty\n",
    "        equivalence_class = get_empty_kernel(V)\n",
    "    \n",
    "    return equivalence_class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def test_eq(p,V,all_possible_p):\n",
    "    eq_test = equivalence_class(p,V)\n",
    "    bool_p = extension_bool(p,V)\n",
    "\n",
    "    match_list = []\n",
    "    for q in eq_test:\n",
    "        bool_test = extension_bool(q,V)\n",
    "        match_list.append(all(bool_p == bool_test))\n",
    "    print all(match_list)\n",
    "    eq_set = set([tuple(q) for q in eq_test])\n",
    "    match_list = []\n",
    "    for q in all_possible_p:\n",
    "        if not q in eq_set:\n",
    "            bool_test = extension_bool(np.array(q),V)\n",
    "            match_list.append(all(bool_p==bool_test))\n",
    "    print any(match_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def verbose_test(p,V,all_possible_p):\n",
    "    bool_p = extension_bool(p,V)\n",
    "    print('The boolean inclusion vector')\n",
    "    print(bool_p)\n",
    "    eq_test = equivalence_class(p,V)\n",
    "    print('The proposed equivalence class')\n",
    "    print(np.stack(eq_test,axis=0))\n",
    "    eq_set = set([tuple(q) for q in eq_test])\n",
    "    \n",
    "    wrongly_included_q = [] #the elements of eq_test that shouldn't be there\n",
    "    wrongly_excluded_q = [] #the psfvs not in eq_test that should be there\n",
    "    \n",
    "    for q in all_possible_p:\n",
    "        bool_test = extension_bool(np.array(q),V)\n",
    "        if q in eq_set:\n",
    "            if not all(bool_p==bool_test):\n",
    "                #shouldn't be there\n",
    "                wrongly_included_q.append(q)\n",
    "        else:\n",
    "            if all(bool_p==bool_test):\n",
    "                #it should be there\n",
    "                wrongly_excluded_q.append(q)\n",
    "    print('wrongly included psfvs')\n",
    "    if len(wrongly_included_q)>0:\n",
    "        print(np.stack(wrongly_included_q,axis=0))\n",
    "    else:\n",
    "        print('none')\n",
    "    print('wrongly excluded psfvs')\n",
    "    if len(wrongly_excluded_q)>0:\n",
    "        print(np.stack(wrongly_excluded_q,axis=0))\n",
    "    else:\n",
    "        print('none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "M = 5\n",
    "L = 10\n",
    "all_feature_vecs = [comb for comb in itertools.product([-1,1],repeat=M)]\n",
    "all_possible_p = [p for p in itertools.product([-1,0,1],repeat=M)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  1,  1,  1,  1],\n",
       "       [ 1,  1,  1,  1, -1],\n",
       "       [ 1, -1, -1, -1,  1],\n",
       "       [-1, -1, -1, -1,  1],\n",
       "       [-1, -1, -1,  1,  1],\n",
       "       [ 1, -1, -1, -1, -1],\n",
       "       [-1, -1, -1, -1, -1],\n",
       "       [ 1,  1,  1, -1, -1],\n",
       "       [ 1,  1, -1,  1, -1],\n",
       "       [ 1,  1, -1, -1,  1]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_list = random.sample(all_feature_vecs,L)\n",
    "V = np.stack(v_list,axis=0)\n",
    "V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0, -1, -1,  1,  0])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_query = np.array(random.choice(all_possible_p))\n",
    "p_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 1 0 0 0 0 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-1, -1, -1,  1,  1]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(extension_bool(p_query,V))\n",
    "ext_mat = extension_mat(p_query,V)\n",
    "ext_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([-1, -1,  0,  1,  0]),\n",
       " array([ 0, -1, -1,  1,  1]),\n",
       " array([-1,  0,  0,  1,  1]),\n",
       " array([ 0, -1, -1,  1,  0]),\n",
       " array([-1,  0, -1,  1,  0]),\n",
       " array([-1, -1, -1,  1,  1]),\n",
       " array([ 0, -1,  0,  1,  0]),\n",
       " array([-1,  0,  0,  1,  0]),\n",
       " array([-1, -1, -1,  1,  0]),\n",
       " array([ 0, -1,  0,  1,  1]),\n",
       " array([-1,  0, -1,  1,  1]),\n",
       " array([-1, -1,  0,  1,  1]),\n",
       " array([ 0,  0, -1,  1,  1])]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "equivalence_class(p_query,V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0, -1, -1,  1,  0])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  1,  1,  1,  1],\n",
       "       [ 1,  1,  1,  1, -1],\n",
       "       [ 1, -1, -1, -1,  1],\n",
       "       [-1, -1, -1, -1,  1],\n",
       "       [-1, -1, -1,  1,  1],\n",
       "       [ 1, -1, -1, -1, -1],\n",
       "       [-1, -1, -1, -1, -1],\n",
       "       [ 1,  1,  1, -1, -1],\n",
       "       [ 1,  1, -1,  1, -1],\n",
       "       [ 1,  1, -1, -1,  1]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The boolean inclusion vector\n",
      "[0 0 0 0 1 0 0 0 0 0]\n",
      "The proposed equivalence class\n",
      "[[-1 -1  0  1  0]\n",
      " [ 0 -1 -1  1  1]\n",
      " [-1  0  0  1  1]\n",
      " [ 0 -1 -1  1  0]\n",
      " [-1  0 -1  1  0]\n",
      " [-1 -1 -1  1  1]\n",
      " [ 0 -1  0  1  0]\n",
      " [-1  0  0  1  0]\n",
      " [-1 -1 -1  1  0]\n",
      " [ 0 -1  0  1  1]\n",
      " [-1  0 -1  1  1]\n",
      " [-1 -1  0  1  1]\n",
      " [ 0  0 -1  1  1]]\n",
      "wrongly included psfvs\n",
      "none\n",
      "wrongly excluded psfvs\n",
      "none\n"
     ]
    }
   ],
   "source": [
    "verbose_test(p_query,V,all_possible_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1, -1,  1,  1, -1]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ext_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({frozenset(),\n",
       "  frozenset({2}),\n",
       "  frozenset({2, 4}),\n",
       "  frozenset({0}),\n",
       "  frozenset({1, 2}),\n",
       "  frozenset({4}),\n",
       "  frozenset({0, 4}),\n",
       "  frozenset({0, 2, 4}),\n",
       "  frozenset({1, 2, 4}),\n",
       "  frozenset({0, 2}),\n",
       "  frozenset({1}),\n",
       "  frozenset({0, 1, 4}),\n",
       "  frozenset({0, 1, 2}),\n",
       "  frozenset({0, 1, 2, 4})},\n",
       " array([[ 1,  1,  1,  1, -1],\n",
       "        [-1, -1,  1, -1, -1],\n",
       "        [ 1, -1,  1, -1,  1],\n",
       "        [-1,  1, -1, -1, -1],\n",
       "        [ 1, -1, -1, -1, -1],\n",
       "        [-1, -1,  1, -1,  1],\n",
       "        [ 1,  1,  1, -1, -1],\n",
       "        [-1,  1,  1,  1,  1],\n",
       "        [-1,  1,  1, -1,  1]]),\n",
       " [frozenset({0, 1}),\n",
       "  frozenset({3}),\n",
       "  frozenset({0, 3, 4}),\n",
       "  frozenset({1, 2, 3}),\n",
       "  frozenset({0, 2, 3}),\n",
       "  frozenset({3, 4}),\n",
       "  frozenset({0, 1, 3}),\n",
       "  frozenset({1, 4}),\n",
       "  frozenset({1, 3, 4})])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_specific_q = np.array([-1, -1,  1,  1, -1])\n",
    "get_allowed_abstraction_sets(most_specific_q,V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 1, 0), (1, -1, 0), (1, 0, 1), (1, 0, -1)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_children([1,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "3\n",
      "6\n",
      "1\n",
      "4\n",
      "7\n",
      "2\n",
      "5\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "for p in itertools.product([-1,0,1],repeat=2):\n",
    "    print trinary_hash_function(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
