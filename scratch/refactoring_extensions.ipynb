{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def heaviside(x):\n",
    "    return np.array(1 * (x >= 0))\n",
    "def powerset(iterable):\n",
    "    \"powerset([1,2,3]) --> () (1,) (2,) (3,) (1,2) (1,3) (2,3) (1,2,3)\"\n",
    "    s = list(iterable)\n",
    "    return list(itertools.chain.from_iterable(itertools.combinations(s, r) for r in range(len(s)+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def extension_bool(p,V):\n",
    "    \"\"\"\n",
    "    Compute a boolean vector that represents the extension of p in V\n",
    "    \n",
    "    Inputs:\n",
    "        p - a vector of shape (M,) with elements from {-1,0,1}.  The partially specified feature vector\n",
    "        V-  a matrix of shape (L,M) with elements from {-1,1}.  The feature vectors\n",
    "    Outputs:\n",
    "        extension - a vector of shape (L) with elements from {1,0}.  extension[l]=1 iff V[l,:] is in the extension of p\n",
    "    \"\"\"\n",
    "    L = np.shape(V)[0]\n",
    "    if np.sum(abs(p)) == 0:\n",
    "        #the all zeros vector\n",
    "        return np.ones(L)\n",
    "    else:\n",
    "        E = np.dot(V,p)/np.sum(abs(p))\n",
    "        return heaviside(E-1)\n",
    "\n",
    "def extension_multi_bool(p_mat,V):\n",
    "    \"\"\"\n",
    "    Compute a boolean vector that represents the extension of p in V\n",
    "    \n",
    "    Inputs:\n",
    "        p_mat - a matrix of shape (M,num_p) with elements from {-1,0,1}.  The matrix of partially specified\n",
    "            feature vectors, containing num_p vectors\n",
    "        V-  a matrix of shape (L,M) with elements from {-1,1}.  The feature vectors\n",
    "    Outputs:\n",
    "        extension - a matrix of shape (L,num_p) with elements from {1,0}.  extension[l,i]=1 iff V[l,:] is \n",
    "            in the extension of p_mat[:,i]\n",
    "    \"\"\"\n",
    "    K_vec = np.sum(abs(p_mat),axis=0) #shape is (num_p,)\n",
    "    E = np.dot(V,p_mat) #shape is (L,num_p)\n",
    "    return heaviside(E-K_vec[np.newaxis,:])\n",
    "\n",
    "def extension_mat(p,V):\n",
    "    \"\"\"\n",
    "    Returns a matrix representing the extension of p in V\n",
    "    \n",
    "    Inputs:\n",
    "        p - a vector of shape (M,) with elements from {-1,0,1}.  The partially specified feature vector\n",
    "        V-  a matrix of shape (L,M) with elements from {-1,1}.  The feature vectors\n",
    "    Outputs:\n",
    "        A - A matrix of shape (N,M) with elements from {-1,1}, where N <=L, and the rows of A are the rows of V in the\n",
    "            extension of p\n",
    "    \"\"\"\n",
    "    \n",
    "    bool_vec = extension_bool(p,V)\n",
    "    return V[bool_vec>0,:]\n",
    "def check_equivalent(p,q,V):\n",
    "    \"\"\"\n",
    "    Checks if p and q have the same extension in V\n",
    "    \n",
    "    Inputs:\n",
    "        p,q - vectors of shape (M,) with elements from {-1,0,1}.  The partially specified feature vectors\n",
    "        V-  a matrix of shape (L,M) with elements from {-1,1}.  The feature vectors\n",
    "    Outputs:\n",
    "        match - bool, whether or not p and q have the same extension \n",
    "    \"\"\"\n",
    "    p_bool = extension_bool(p,V)\n",
    "    q_bool = extension_bool(q,V)\n",
    "    M = np.shape(q_bool)[0]\n",
    "    return all([p_bool[i]==q_bool[i] for i in range(M)])\n",
    "\n",
    "def mutate_p(p,abstraction_set,specification_set):\n",
    "    \"\"\"\n",
    "    Returns q that is equal to p with mutations (specifications and generalizations applied)\n",
    "    \n",
    "    Inputs:\n",
    "        p - the partially specified feature vector to be mutated\n",
    "        abstraction_set - a set of indices to flip currently specified values of p to zero\n",
    "        specification_set - a set of tuples (index,value) to change currently unspecified values of p to the given value\n",
    "    Outputs:\n",
    "        q - the mutated vector\n",
    "    \"\"\"\n",
    "    \n",
    "    q = np.copy(p)\n",
    "    for abstraction in abstraction_set:\n",
    "        q[abstraction]=0\n",
    "    for specification_tuple in specification_set:\n",
    "        q[specification_tuple[0]]=specification_tuple[1]\n",
    "    return q\n",
    "\n",
    "def get_allowed_specifications(p,V):\n",
    "    \"\"\"\n",
    "    Given a set of feature vectors V and a query partially specified feature vector p, all locations where \n",
    "    p can be specified (flipped from a zero to +/- 1) without changing the extension in V.\n",
    "    \n",
    "    Inputs:\n",
    "        p - a vector of shape (M,) with elements from {-1,0,1}.  The partially specified feature vector\n",
    "        V -  a matrix of shape (L,M) with elements from {-1,1}.  The feature vectors\n",
    "    Outputs:\n",
    "        allowed_specifications - a list of tuples of form (index,value) where p[index] can be set to value\n",
    "        without changing extension\n",
    "    \"\"\"\n",
    "    \n",
    "    M = np.shape(p)[0]\n",
    "    \n",
    "    bool_p = extension_bool(p,V)\n",
    "    \n",
    "    A = V[bool_p>0,:]#shape is (N,M)\n",
    "    N = np.shape(A)[0]\n",
    "    A_sums = np.sum(A,axis=0)\n",
    "    \n",
    "    allowed_specifications = []\n",
    "    for i in range(M):\n",
    "        if p[i]==0:\n",
    "            #we can flip this position to a signed value if only if all A[:,i] or equivalently A_sums[i]== +/- N\n",
    "            if N==0:\n",
    "                #empty extension means we can replace all zeros with +1 or -1\n",
    "                allowed_specifications.append((i,1))\n",
    "                allowed_specifications.append((i,-1))              \n",
    "            elif A_sums[i]==N:\n",
    "                allowed_specifications.append((i,1)) #they're all 1 so we can mutate p[i] to 1\n",
    "            elif A_sums[i]==-N:\n",
    "                allowed_specifications.append((i,-1)) #they're all -1 so we can mutate p[i] to -1\n",
    "    return allowed_specifications\n",
    "    \n",
    "def get_allowed_abstraction_sets(p,V):\n",
    "    \"\"\"\n",
    "    Given a set of feature vectors V and a query partially specified feature vector p, return all partially\n",
    "    specified vectors q that have the same extension in V as p.\n",
    "    \n",
    "    Inputs:\n",
    "        p - a vector of shape (M,) with elements from {-1,0,1}.  The partially specified feature vector\n",
    "        V -  a matrix of shape (L,M) with elements from {-1,1}.  The feature vectors\n",
    "    Outputs:\n",
    "        allowed_abstraction_sets - a list of sets of indices, where each set denotes a set of indices of p\n",
    "            where a previously specified value can be flipped to zero without changing extension\n",
    "    \"\"\"\n",
    "    bool_p = extension_bool(p,V)\n",
    "    B = V[bool_p<=0,:]#shape is (T,M), p is not compatible with any vector in B\n",
    "    (T,M) = np.shape(B)\n",
    "    \n",
    "    \n",
    "    prespecified_indices = [i for i in range(M) if p[i] != 0]\n",
    "    \n",
    "    #now we find all subsets J of the specified indices such that if we zeroed out every index of p contained in J, the\n",
    "    #new vector would be consistent with an element of B\n",
    "    #this accounts to looking at each element b (row) of B, and finding all the indices where p disagrees \n",
    "    #with that element and adding that set of indices to an accumulating set\n",
    "    forbidden_index_subsets_ = []\n",
    "    \n",
    "    for t_val in range(T):\n",
    "        clash_points = []\n",
    "        for m_val in prespecified_indices:\n",
    "            if B[t_val,m_val]*p[m_val] == -1:\n",
    "                clash_points.append(m_val)\n",
    "        forbidden_index_subsets_.append(frozenset(clash_points))\n",
    "    forbidden_index_subsets = set(forbidden_index_subsets_)\n",
    "    \n",
    "    #we can trim this down though by noting that any elment of forbidden_index_subsets that is a \n",
    "    #proper superset of another element of forbidden_index subsets is redundant\n",
    "    \"\"\"\n",
    "    reduced_forbidden_index_subsets = set()\n",
    "    for J in forbidden_index_subsets:\n",
    "        retain = True\n",
    "        for J_prime in forbidden_index_subsets:\n",
    "            if J > J_prime: #J is a proper superset of J_prime\n",
    "                retain=False\n",
    "                break\n",
    "        if retain:\n",
    "            reduced_forbidden_index_subsets.add(J)\n",
    "    \"\"\"\n",
    "    #now we try and build up all the legal subsets of the prespecified indices using a sort of breadth first\n",
    "    #search.  We start with all the legal singletons\n",
    "    \n",
    "    allowed_abstraction_sets=set() #since not abstracting any indices is a legal mutation\n",
    "    my_queue = []\n",
    "    allowed_singletons = []\n",
    "    for i in prespecified_indices:\n",
    "        trial_singleton = frozenset([i])\n",
    "        if not trial_singleton in forbidden_index_subsets:\n",
    "            my_queue.append(trial_singleton)\n",
    "            allowed_singletons.append(trial_singleton)\n",
    "    \n",
    "    #now we will build up allowed sets from elements of the queue\n",
    "    \n",
    "    while len(my_queue)>0:\n",
    "        trial_set = my_queue.pop(0)\n",
    "        if not (trial_set in forbidden_index_subsets):\n",
    "            allowed_abstraction_sets.add(trial_set)\n",
    "            #now we combine this allowed set with all the allowed singletons it doesn't contain and add to queue\n",
    "            #for later processing\n",
    "            for trial_singleton in allowed_singletons:\n",
    "                if not (trial_set >= trial_singleton): #the trial set doesn't already contain this singleton\n",
    "                    new_set = trial_set.union(trial_singleton)\n",
    "                    my_queue.append(new_set)\n",
    "            \n",
    "    allowed_abstraction_sets.add(frozenset())\n",
    "    return allowed_abstraction_sets#,forbidden_index_subsets\n",
    "    \n",
    "def equivalence_class(p,V):\n",
    "    \"\"\"\n",
    "    Given a set of feature vectors V and a query partially specified feature vector p, return all partially\n",
    "    specified vectors q that have the same extension in V as p.\n",
    "    \n",
    "    Inputs:\n",
    "        p - a vector of shape (M,) with elements from {-1,0,1}.  The partially specified feature vector\n",
    "        V-  a matrix of shape (L,M) with elements from {-1,1}.  The feature vectors\n",
    "    Outputs:\n",
    "        R - a list of vectors where R[i] has shape (M,) and all vectors in R have the same extension in V as p\n",
    "    \"\"\"\n",
    "    \n",
    "    M = np.shape(p)[0]\n",
    "    L = np.shape(V)[0]\n",
    "    V_sums = np.sum(V,axis=0)\n",
    "    bool_p = extension_bool(p,V)\n",
    "    \n",
    "    allowed_specifications = get_allowed_specifications(p,V)\n",
    "    allowed_specification_sets = powerset(allowed_specifications) #nope, can't have (i,1) (i,-1) in same\n",
    "    allowed_abstraction_sets = get_allowed_abstraction_sets(p,V)\n",
    "    \n",
    "    equivalence_class = []\n",
    "    \n",
    "    for specification_set in allowed_specification_sets:\n",
    "        for abstraction_set in allowed_abstraction_sets:\n",
    "            #print abstraction_set\n",
    "            q = mutate_p(p,abstraction_set,specification_set)\n",
    "            equivalence_class.append(q)\n",
    "    \n",
    "    #return equivalence_class,allowed_abstraction_sets,allowed_specification_sets\n",
    "    return equivalence_class\n",
    "    #return allowed_specification_sets,legal_index_sets    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def test_eq(p,V,all_possible_p):\n",
    "    eq_test = equivalence_class(p,V)\n",
    "    bool_p = extension_bool(p,V)\n",
    "\n",
    "    match_list = []\n",
    "    for q in eq_test:\n",
    "        bool_test = extension_bool(q,V)\n",
    "        match_list.append(all(bool_p == bool_test))\n",
    "    print all(match_list)\n",
    "    eq_set = set([tuple(q) for q in eq_test])\n",
    "    match_list = []\n",
    "    for q in all_possible_p:\n",
    "        if not q in eq_set:\n",
    "            bool_test = extension_bool(np.array(q),V)\n",
    "            match_list.append(all(bool_p==bool_test))\n",
    "    print any(match_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def verbose_test(p,V,all_possible_p):\n",
    "    bool_p = extension_bool(p,V)\n",
    "    print('The boolean inclusion vector')\n",
    "    print(bool_p)\n",
    "    eq_test = equivalence_class(p,V)\n",
    "    print('The proposed equivalence class')\n",
    "    print(np.stack(eq_test,axis=0))\n",
    "    eq_set = set([tuple(q) for q in eq_test])\n",
    "    \n",
    "    wrongly_included_q = [] #the elements of eq_test that shouldn't be there\n",
    "    wrongly_excluded_q = [] #the psfvs not in eq_test that should be there\n",
    "    \n",
    "    for q in all_possible_p:\n",
    "        bool_test = extension_bool(np.array(q),V)\n",
    "        if q in eq_set:\n",
    "            if not all(bool_p==bool_test):\n",
    "                #shouldn't be there\n",
    "                wrongly_included_q.append(q)\n",
    "        else:\n",
    "            if all(bool_p==bool_test):\n",
    "                #it should be there\n",
    "                wrongly_excluded_q.append(q)\n",
    "    print('wrongly included psfvs')\n",
    "    if len(wrongly_included_q)>0:\n",
    "        print(np.stack(wrongly_included_q,axis=0))\n",
    "    else:\n",
    "        print('none')\n",
    "    print('wrongly excluded psfvs')\n",
    "    if len(wrongly_excluded_q)>0:\n",
    "        print(np.stack(wrongly_excluded_q,axis=0))\n",
    "    else:\n",
    "        print('none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "M = 5\n",
    "L = 10\n",
    "all_feature_vecs = [comb for comb in itertools.product([-1,1],repeat=M)]\n",
    "all_possible_p = [p for p in itertools.product([-1,0,1],repeat=M)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1,  1, -1, -1, -1],\n",
       "       [ 1, -1, -1,  1, -1],\n",
       "       [-1, -1, -1, -1,  1],\n",
       "       [ 1,  1, -1, -1, -1],\n",
       "       [ 1,  1,  1,  1,  1],\n",
       "       [-1,  1,  1, -1, -1],\n",
       "       [-1,  1,  1, -1,  1],\n",
       "       [ 1, -1,  1, -1, -1],\n",
       "       [-1, -1,  1, -1,  1],\n",
       "       [ 1,  1,  1, -1, -1]])"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_list = random.sample(all_feature_vecs,L)\n",
    "V = np.stack(v_list,axis=0)\n",
    "V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  1,  0, -1, -1])"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_query = np.array(random.choice(all_possible_p))\n",
    "p_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 1 0 0 0 0 0 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1,  1, -1, -1, -1],\n",
       "       [ 1,  1,  1, -1, -1]])"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(extension_bool(p_query,V))\n",
    "ext_mat = extension_mat(p_query,V)\n",
    "ext_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0, 1, 0, 0, 1]),\n",
       " array([0, 1, 1, 0, 0]),\n",
       " array([-1,  1,  1,  0,  1]),\n",
       " array([ 0,  1,  1, -1,  1]),\n",
       " array([ 0,  1,  1, -1,  0]),\n",
       " array([-1,  1,  0,  0,  0]),\n",
       " array([ 0,  1,  0, -1,  0]),\n",
       " array([-1,  1,  1, -1,  0]),\n",
       " array([-1,  1,  0,  0,  1]),\n",
       " array([-1,  1,  0, -1,  1])]"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "equivalence_class(p_query,V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1,  1, -1,  0,  1])"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1,  1,  1,  1, -1],\n",
       "       [ 1, -1, -1, -1, -1],\n",
       "       [ 1,  1,  1,  1,  1],\n",
       "       [ 1, -1,  1,  1, -1],\n",
       "       [ 1,  1, -1,  1, -1],\n",
       "       [-1,  1, -1,  1, -1],\n",
       "       [-1,  1, -1, -1,  1],\n",
       "       [-1, -1,  1,  1, -1],\n",
       "       [-1, -1,  1, -1, -1],\n",
       "       [-1,  1, -1, -1, -1]])"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The boolean inclusion vector\n",
      "[0 0 0 0 0 0 1 0 0 0]\n",
      "The proposed equivalence class\n",
      "[[ 0  1 -1  0  1]\n",
      " [-1  0  0  0  1]\n",
      " [-1  1 -1  0  1]\n",
      " [-1  0 -1  0  1]\n",
      " [ 0  0  0  0  1]\n",
      " [-1  1  0  0  1]\n",
      " [ 0  0 -1  0  1]\n",
      " [ 0  1 -1 -1  1]\n",
      " [-1  0  0 -1  1]\n",
      " [-1  1 -1 -1  1]\n",
      " [-1  0 -1 -1  1]\n",
      " [ 0  0  0 -1  1]\n",
      " [-1  1  0 -1  1]\n",
      " [ 0  0 -1 -1  1]]\n",
      "wrongly included psfvs\n",
      "[[0 0 0 0 1]]\n",
      "wrongly excluded psfvs\n",
      "[[ 0  1  0 -1  1]]\n"
     ]
    }
   ],
   "source": [
    "verbose_test(p_query,V,all_possible_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "v_list = random.sample(all_feature_vecs,L)\n",
    "V = np.stack(v_list,axis=0)\n",
    "p_query = np.array(random.choice(all_possible_p))\n",
    "test_eq(p_query,V,all_possible_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1,  1, -1, -1, -1],\n",
       "       [-1,  1,  1, -1,  1],\n",
       "       [ 1,  1, -1, -1,  1],\n",
       "       [-1, -1,  1,  1,  1],\n",
       "       [ 1,  1, -1,  1,  1],\n",
       "       [ 1,  1,  1, -1,  1],\n",
       "       [ 1,  1, -1, -1, -1],\n",
       "       [ 1,  1,  1,  1,  1],\n",
       "       [-1, -1, -1, -1,  1],\n",
       "       [-1,  1, -1, -1,  1]])"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], shape=(0, 5), dtype=int64)"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extension_mat(p_query,V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1, -1,  1, -1,  0])"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{frozenset(), frozenset({0})}"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_allowed_abstraction_sets(p_query,V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4, 1), (4, -1)]"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_allowed_specifications(p_query,V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 1, -1,  1, -1,  0]),\n",
       " array([ 0, -1,  1, -1,  0]),\n",
       " array([ 1, -1,  1, -1,  1]),\n",
       " array([ 0, -1,  1, -1,  1]),\n",
       " array([ 1, -1,  1, -1, -1]),\n",
       " array([ 0, -1,  1, -1, -1]),\n",
       " array([ 1, -1,  1, -1, -1]),\n",
       " array([ 0, -1,  1, -1, -1])]"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "equivalence_class(p_query,V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = set()\n",
    "a.add(frozenset())\n",
    "len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
